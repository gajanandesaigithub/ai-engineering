- Masked language model:
	- This model refers the preceding and succeeding tokens/context and predicts the missing token in between. Acts to fill the blanks.
	- Useful in sentiment analysis, text classification.
	
- Autoregressive language models
	- This model only refers the preceding tokens and predicts the next token. It can keep continuing generating next token one after other.
	- Outputs are open ended, i.e. generates open ended output, hence they are called generative models.
	- Remember, language model doesn’t choose "right" token. It chooses "most probable" token from the given training data. The model learns not what is possible, but what is statistically likely.
		- This is important because 
			- models fail in low-frequency cases
			- rare truths get suppressed
			- creativity comes from sampling, not from knowledge
			- Hence RAG is needed for correctness

That’s why:

hallucinations happen

rare but valid answers may be missed

bias reflects the data
	
- BPE = Byte Pair Encoding
	- Its a way to break words into reusable subword pieces so the vocabulary stays small but expressive.
	- BPE builds a vocabulary of commonly occurring fragments so models can reuse them efficiently.
	- E.g.
		Without knowing linguistics, BPE discovers:
			prefixes: un-, dis-
			suffixes: -ing, -ed, -ly
			roots: believe, nation, compute
		Because they occur often.
		Now, when it sees unknown work like - "unbelievableness", BPE tokenizes as: un + believe + able + ness
		No <UNK> needed, meaning is approximated from parts.
		Note: <UNK> = unknown = not present in model's vocabulary. 
		
		
		
		

		
-----------------------------------------------------	
	Parking area (pending queries)
-----------------------------------------------------
- BPE vs WordPiece vs Unigram (practical differences)